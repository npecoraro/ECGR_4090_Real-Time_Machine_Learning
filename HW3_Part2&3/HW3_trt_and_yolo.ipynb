{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "involved-train",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import trt_pose.coco\n",
    "\n",
    "with open('human_pose.json', 'r') as f:\n",
    "    human_pose = json.load(f)\n",
    "\n",
    "topology = trt_pose.coco.coco_category_to_topology(human_pose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "utility-replication",
   "metadata": {},
   "outputs": [],
   "source": [
    "import trt_pose.models\n",
    "\n",
    "num_parts = len(human_pose['keypoints'])\n",
    "num_links = len(human_pose['skeleton'])\n",
    "\n",
    "model = trt_pose.models.resnet18_baseline_att(num_parts, 2 * num_links).cuda().eval()\n",
    "#model = trt_pose.models.densenet121_baseline_att(num_parts, 2 * num_links).cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "brutal-movie",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "MODEL_WEIGHTS = 'resnet18_baseline_att_224x224_A_epoch_249.pth'\n",
    "#MODEL_WEIGHTS = 'densenet121_baseline_att_256x256_B_epoch_160.pth'\n",
    "\n",
    "model.load_state_dict(torch.load(MODEL_WEIGHTS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "external-batman",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/nathan/.cache/torch/hub/ultralytics_yolov5_master\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Focus                     [3, 32, 3]                    \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  1    156928  models.common.C3                        [128, 128, 3]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  1    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1    656896  models.common.SPP                       [512, 512, [5, 9, 13]]        \n",
      "  9                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1    229245  models.yolo.Detect                      [80, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
      "Model Summary: 283 layers, 7276605 parameters, 7276605 gradients\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding autoShape... \n"
     ]
    }
   ],
   "source": [
    "modelYolo = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dress-rental",
   "metadata": {},
   "outputs": [],
   "source": [
    "WIDTH = 224\n",
    "HEIGHT = 224\n",
    "\n",
    "#WIDTH = 256\n",
    "#HEIGHT = 256\n",
    "\n",
    "data = torch.zeros((1, 3, HEIGHT, WIDTH)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "featured-reduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import cv2\n",
    "import torchvision.transforms as transforms\n",
    "import PIL.Image\n",
    "\n",
    "mean = torch.Tensor([0.485, 0.456, 0.406]).cuda()\n",
    "std = torch.Tensor([0.229, 0.224, 0.225]).cuda()\n",
    "device = torch.device('cuda')\n",
    "\n",
    "def preprocess(image):\n",
    "    global device\n",
    "    device = torch.device('cuda')\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = PIL.Image.fromarray(image)\n",
    "    image = transforms.functional.to_tensor(image).to(device)\n",
    "    image.sub_(mean[:, None, None]).div_(std[:, None, None])\n",
    "    return image[None, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "super-stick",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trt_pose.draw_objects import DrawObjects\n",
    "from trt_pose.parse_objects import ParseObjects\n",
    "\n",
    "parse_objects = ParseObjects(topology)\n",
    "draw_objects = DrawObjects(topology)\n",
    "resize = transforms.Compose([transforms.ToPILImage(),transforms.Resize((HEIGHT, WIDTH))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "future-render",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This took  109.35719609260559\n",
      "This took  12.527987480163574\n",
      "This took  3.7289042472839355\n",
      "This took  6.236683130264282\n",
      "This took  18.82216453552246\n",
      "This took  23.97421956062317\n",
      "This took  4.771433591842651\n",
      "This took  5.550743103027344\n",
      "This took  4.3053998947143555\n",
      "This took  6.556941986083984\n",
      "This took  5.101637601852417\n",
      "This took  4.723688125610352\n",
      "This took  4.503425598144531\n",
      "This took  7.05120587348938\n",
      "This took  4.975395679473877\n",
      "This took  9.608513116836548\n",
      "This took  13.935269594192505\n",
      "This took  6.488142967224121\n",
      "This took  7.592902660369873\n",
      "This took  10.196940183639526\n",
      "This took  3.6204299926757812\n",
      "This took  6.24502158164978\n",
      "This took  6.269418954849243\n",
      "This took  9.237916707992554\n",
      "This took  7.572328567504883\n",
      "This took  6.968639373779297\n",
      "This took  8.68227243423462\n",
      "This took  10.764209985733032\n"
     ]
    }
   ],
   "source": [
    "vid = cv2.VideoCapture('HumanPose1.mp4')\n",
    "vidRunning = True\n",
    "    \n",
    "while(vidRunning):\n",
    "    tup, frame = vid.read()\n",
    "    if tup:\n",
    "        resize(frame)\n",
    "        data = preprocess(frame)\n",
    "        t0 = time.time()\n",
    "        cmap, paf = model(data)\n",
    "        t1 = time.time()\n",
    "        \n",
    "        cmap, paf = cmap.detach().cpu(), paf.detach().cpu()\n",
    "        counts, objects, peaks = parse_objects(cmap, paf)\n",
    "        \n",
    "        t2 = time.time()\n",
    "        results = modelYolo(frame)\n",
    "        t3 = time.time()\n",
    "        \n",
    "        quickMath = ((t1-t0) + (t3-t2))\n",
    "        print(\"This took \", quickMath)\n",
    "        \n",
    "        draw_objects(frame, counts, objects, peaks)\n",
    "        \n",
    "        returned = results.render()\n",
    "        finalImage = cv2.add(frame, returned[0])\n",
    "        cv2.imshow('OUTPUT', finalImage)\n",
    "        cv2.waitKey(1)\n",
    "        \n",
    "    else:\n",
    "        vidRunning = False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
